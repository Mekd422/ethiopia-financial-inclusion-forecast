{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Data Exploration and Enrichment\n",
        "\n",
        "## Objective\n",
        "Understand the starter dataset and enrich it with additional data useful for forecasting financial inclusion in Ethiopia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = Path('../data/raw')\n",
        "PROCESSED_DIR = Path('../data/processed')\n",
        "PROCESSED_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the unified dataset\n",
        "df = pd.read_csv(DATA_DIR / 'ethiopia_fi_unified_data.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load reference codes\n",
        "ref_codes = pd.read_csv(DATA_DIR / 'reference_codes.csv')\n",
        "print(f\"Reference codes shape: {ref_codes.shape}\")\n",
        "ref_codes.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understand the Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count records by record_type\n",
        "print(\"=== Record Type Distribution ===\")\n",
        "record_counts = df['record_type'].value_counts()\n",
        "print(record_counts)\n",
        "print(f\"\\nTotal records: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore structure by record type\n",
        "for record_type in df['record_type'].unique():\n",
        "    print(f\"\\n=== {record_type.upper()} Records ===\")\n",
        "    subset = df[df['record_type'] == record_type]\n",
        "    print(f\"Count: {len(subset)}\")\n",
        "    if len(subset) > 0:\n",
        "        print(f\"\\nSample record:\")\n",
        "        print(subset.iloc[0].to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check pillar distribution\n",
        "if 'pillar' in df.columns:\n",
        "    print(\"=== Pillar Distribution ===\")\n",
        "    print(df['pillar'].value_counts())\n",
        "    \n",
        "# Check source_type distribution\n",
        "if 'source_type' in df.columns:\n",
        "    print(\"\\n=== Source Type Distribution ===\")\n",
        "    print(df['source_type'].value_counts())\n",
        "    \n",
        "# Check confidence distribution\n",
        "if 'confidence' in df.columns:\n",
        "    print(\"\\n=== Confidence Distribution ===\")\n",
        "    print(df['confidence'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Temporal Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify temporal range\n",
        "date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
        "print(f\"Date columns found: {date_cols}\")\n",
        "\n",
        "for col in date_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Range: {df[col].min()} to {df[col].max()}\")\n",
        "        print(f\"  Non-null count: {df[col].notna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Indicator Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all unique indicators\n",
        "if 'indicator_code' in df.columns:\n",
        "    indicators = df[df['record_type'] == 'observation']['indicator_code'].unique()\n",
        "    print(f\"Unique indicators: {len(indicators)}\")\n",
        "    print(\"\\nIndicators:\")\n",
        "    for ind in sorted(indicators):\n",
        "        count = len(df[(df['record_type'] == 'observation') & (df['indicator_code'] == ind)])\n",
        "        print(f\"  {ind}: {count} observations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Event Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore events\n",
        "events = df[df['record_type'] == 'event'].copy()\n",
        "print(f\"Total events: {len(events)}\")\n",
        "\n",
        "if 'category' in events.columns:\n",
        "    print(\"\\nEvent categories:\")\n",
        "    print(events['category'].value_counts())\n",
        "\n",
        "if 'event_date' in events.columns or 'observation_date' in events.columns:\n",
        "    date_col = 'event_date' if 'event_date' in events.columns else 'observation_date'\n",
        "    events[date_col] = pd.to_datetime(events[date_col], errors='coerce')\n",
        "    print(\"\\nEvents timeline:\")\n",
        "    print(events[[date_col, 'category']].sort_values(date_col))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Impact Links Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore impact_links\n",
        "impact_links = df[df['record_type'] == 'impact_link'].copy()\n",
        "print(f\"Total impact links: {len(impact_links)}\")\n",
        "\n",
        "if 'parent_id' in impact_links.columns:\n",
        "    print(\"\\nImpact links by event:\")\n",
        "    print(impact_links['parent_id'].value_counts())\n",
        "\n",
        "if 'impact_direction' in impact_links.columns:\n",
        "    print(\"\\nImpact directions:\")\n",
        "    print(impact_links['impact_direction'].value_counts())\n",
        "    \n",
        "if 'pillar' in impact_links.columns:\n",
        "    print(\"\\nImpact links by pillar:\")\n",
        "    print(impact_links['pillar'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=== Missing Values by Column ===\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing %': missing_pct\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Data Enrichment\n",
        "\n",
        "In this section, we'll add new observations, events, and impact_links that will be useful for forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create enriched dataset\n",
        "df_enriched = df.copy()\n",
        "\n",
        "# Track new records\n",
        "new_records = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Add Additional Observations\n",
        "\n",
        "Based on the Additional Data Points Guide, we'll add:\n",
        "- Infrastructure data (4G coverage, mobile penetration, ATM density)\n",
        "- Active account metrics\n",
        "- Agent network data\n",
        "- Transaction volume data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Add infrastructure observations\n",
        "# Note: Replace with actual data from sources like ITU, GSMA, NBE reports\n",
        "\n",
        "def add_observation(record_type, pillar, indicator, indicator_code, value_numeric, \n",
        "                   observation_date, source_name, source_url, confidence='medium',\n",
        "                   original_text='', notes='', collected_by='Data Team', collection_date='2026-02-01'):\n",
        "    \"\"\"Helper function to add new observations following the schema\"\"\"\n",
        "    new_record = {\n",
        "        'record_type': record_type,\n",
        "        'pillar': pillar,\n",
        "        'indicator': indicator,\n",
        "        'indicator_code': indicator_code,\n",
        "        'value_numeric': value_numeric,\n",
        "        'observation_date': observation_date,\n",
        "        'source_name': source_name,\n",
        "        'source_url': source_url,\n",
        "        'confidence': confidence,\n",
        "        'original_text': original_text,\n",
        "        'notes': notes,\n",
        "        'collected_by': collected_by,\n",
        "        'collection_date': collection_date\n",
        "    }\n",
        "    return new_record\n",
        "\n",
        "# Add sample infrastructure observations (replace with real data)\n",
        "# These are examples - you should replace with actual data from ITU, GSMA, NBE, etc.\n",
        "\n",
        "print(\"Adding new observations...\")\n",
        "print(\"Note: Replace sample data with actual data from sources\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Add Additional Events\n",
        "\n",
        "Add important events that aren't yet captured:\n",
        "- Regulatory changes\n",
        "- Infrastructure investments\n",
        "- Partnership announcements\n",
        "- Market milestones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_event(record_type, category, event_date, event_name, description,\n",
        "             source_name, source_url, confidence='medium', original_text='',\n",
        "             notes='', collected_by='Data Team', collection_date='2026-02-01'):\n",
        "    \"\"\"Helper function to add new events following the schema\"\"\"\n",
        "    new_record = {\n",
        "        'record_type': record_type,\n",
        "        'category': category,\n",
        "        'event_date': event_date,\n",
        "        'event_name': event_name,\n",
        "        'description': description,\n",
        "        'source_name': source_name,\n",
        "        'source_url': source_url,\n",
        "        'confidence': confidence,\n",
        "        'original_text': original_text,\n",
        "        'notes': notes,\n",
        "        'collected_by': collected_by,\n",
        "        'collection_date': collection_date\n",
        "        # Note: pillar should be left empty for events\n",
        "    }\n",
        "    return new_record\n",
        "\n",
        "print(\"Adding new events...\")\n",
        "print(\"Note: Replace sample data with actual events from news, reports, etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Add Additional Impact Links\n",
        "\n",
        "Create relationships between events and indicators based on:\n",
        "- Comparable country evidence\n",
        "- Expert analysis\n",
        "- Historical patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_impact_link(record_type, parent_id, pillar, related_indicator, \n",
        "                    impact_direction, impact_magnitude, lag_months, evidence_basis,\n",
        "                    source_name, source_url, confidence='medium', original_text='',\n",
        "                    notes='', collected_by='Data Team', collection_date='2026-02-01'):\n",
        "    \"\"\"Helper function to add new impact links following the schema\"\"\"\n",
        "    new_record = {\n",
        "        'record_type': record_type,\n",
        "        'parent_id': parent_id,\n",
        "        'pillar': pillar,\n",
        "        'related_indicator': related_indicator,\n",
        "        'impact_direction': impact_direction,\n",
        "        'impact_magnitude': impact_magnitude,\n",
        "        'lag_months': lag_months,\n",
        "        'evidence_basis': evidence_basis,\n",
        "        'source_name': source_name,\n",
        "        'source_url': source_url,\n",
        "        'confidence': confidence,\n",
        "        'original_text': original_text,\n",
        "        'notes': notes,\n",
        "        'collected_by': collected_by,\n",
        "        'collection_date': collection_date\n",
        "    }\n",
        "    return new_record\n",
        "\n",
        "print(\"Adding new impact links...\")\n",
        "print(\"Note: Replace with actual impact estimates based on evidence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Enriched Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert new records to DataFrame and append\n",
        "if new_records:\n",
        "    new_df = pd.DataFrame(new_records)\n",
        "    # Ensure all columns match\n",
        "    for col in df_enriched.columns:\n",
        "        if col not in new_df.columns:\n",
        "            new_df[col] = None\n",
        "    df_enriched = pd.concat([df_enriched, new_df], ignore_index=True)\n",
        "    print(f\"Added {len(new_records)} new records\")\n",
        "    print(f\"Total records: {len(df_enriched)}\")\n",
        "else:\n",
        "    print(\"No new records added yet. Add records using the helper functions above.\")\n",
        "\n",
        "# Save enriched dataset\n",
        "df_enriched.to_csv(PROCESSED_DIR / 'ethiopia_fi_unified_data_enriched.csv', index=False)\n",
        "print(f\"\\nEnriched dataset saved to {PROCESSED_DIR / 'ethiopia_fi_unified_data_enriched.csv'}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
